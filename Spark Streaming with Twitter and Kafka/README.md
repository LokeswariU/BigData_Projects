**Dataset:**
-	Create a Twitter account. Create a developer account in Twitter. 
- Create an App in Twitter developer account and set up an dev environment to retrieve the tweets. 
-	Use the Access key, Secret key, Token Access key and Token Secret key to retrieve the tweets using TwitterUtils.createStream() function.

**Steps to Implement**
1. Download Kafka, Apache_spark, ElasticSearch, Kibana and Logstash.
2. Follow all the commands step by step given in the Word Document.
3. Open Command Prompt to start Zookeeper and Kafka.Create the Producer and Consumer window in seperate command prompt. 
4. Check the connectivity between the Producer and Consumer.
5. Using Spark run the Jar file of the scala code.
6. Run Elasticsearch, kibana and logstash. 
7. Open the URL mentioned in the document to open the kibana website.
8. Visualize the graph based on the particular time period by giving the time period and the interval. Start the graph, this will output the sentiment of the each tweet analysed in that particular time limit e.g. 2 hours.
9.  Check the sample graph in the word Document.
10. The Jar file is uploaded in the S3 bucket in AWS and made public to get the URL.
"https://assignment-3-bigdata.s3.amazonaws.com/kafka-assembly-0.1.jar". The kafka.zip is the project folder without the jar file which is run using IntelliJ.


**Implementation:**
1. Fetching the data from the Twitter and inputting it to the Producer is implemented in scala code. Then the Sentiment for each tweet is retrieved and display in the Consumer window of the kafka. 
2. The scala code is compiled and the jar file is generated using IntelliJ IDEA. The fat jar file is generated under JDK – 12.0.2 version, Scala – 2.11.12, Spark – 2.4.5 version.
3. The jar file is generated by running sbt first. In the sbt pane, below project name -> sbt tasks -> click assembly to generated a fat jar file.
4. The project folder kafka which is run using IntelliJ is also  zipped and attached in the Assignment3 zip folder.
Execution: Run the following commands in the command prompt using Apache Spark and Kafka. Kafka – 2.12-2.5.0 version and Spark – 2.4.5 version.
5. Start and Run Zookeeper in the command prompt in Kafka path   -    
> ***.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties***
 
6. Start and Run Kafka in a new command prompt-
> ***.\bin\windows\kafka-server-start.bat .\config\server.properties***

7. Create Topic in Zookeeper in a new command prompt-
> ***.\bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic kafkaTopic***

8. Start Producer using kafkaTopic	-
> ***.\bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic kafkaTopic***

9. Start Consumer in a new Command prompt –
> ***.\bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic kafkaTopic --from-beginning***
 
10. Open a new Command prompt go to spark bin file path-
> ***spark-submit --class TwitterKafka  Path_to_jar_file\kafka-assembly-0.1.jar “lockdown” kafkaTopic***

11.	This will output the sentiments for each twitter in the consumer command prompt.

12. Open a new command prompt goto Elasticsearch bin file path –
> ***.\elasticsearch***

12.	Open a new command prompt goto Kibana bin file path –
> ***.\kibana***

13. Open a new command prompt goto logstash bin file path –
> ***logstash -f logstash-simple.conf***

14. Save the following command in the logstash-simple.conf file in the bin folder before running the command in the command prompt.
> ***input {***

> ***kafka {***

> ***bootstrap_servers => "localhost:9092"***

> ***topics => ["YourTopic"]***

> ***} }***

> ***output {***

> ***elasticsearch {***

> ***hosts => ["localhost:9200"]***

> ***index => "YourTopic-index"***

> ***} }***
 
15.	After starting Elastic Search and Kibana got to this link address . *http://localhost:5601/*

16.	In the visualization of the graphs, give the Time duration for fetching the tweets and running the project. This gives a graph of sentiments for each tweets in the time duration given as input.

**Input and Output Public paths:**

- The jar file has been uploaded to the S3 bucket in AWS and make as a public path:
*https://assignment-3-bigdata.s3.amazonaws.com/kafka-assembly-0.1.jar*

- Use this link in the Kibana to visualize the Graph of sentiments in three categories -Positive, Negative and Neutral and with filter of 2 hours of implementation: *http://localhost:5601/app/timelion#?_g=(refreshInterval:(pause:!t,value:0),time:(from:now-2h,to:now))&_a=(columns:2,interval:'1m',rows:2,selected:0,sheet:!('.es(q%3D!'message.keyword:NEGATIVE!').label(!'Negative!'),%0A.es(q%3D!'message.keyword:POSITIVE!').label(!'Positive!'),%0A.es(q%3D!'message.keyword:NEUTRAL!').label(!'Neutral!')'))*
- After visualization, the graph displayed will be of this 

 
**Summary:**
1. The sentiments retrieved while analyzing the “lockdown” tweets has more negative emotions. People have expressed more negative sentiments in their tweets retrieved with a timespan of 1 hour which related to lockdown.
2. This is the graph of tweets retrieved during a timespan of 1 hour. The tweets posted which are related to lockdown are more negative rather than positive tweets. The negative sentiment is analyzed only if the sentiment goes below 1(or equal to 0).
3. The neural tweets are also comparatively higher than the positive tweets. Thus we can summarize that the lockdown tweets are more negative and sad.
